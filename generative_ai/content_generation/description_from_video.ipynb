{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate descriptions from videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to generate descriptions of videos in a GCS bucket.  \n",
    "It uses the [Youtube UGC dataset](https://media.withyoutube.com/) and uses the [Gemini](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini) to generate video descriptions for each video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### **Steps**\n",
    "Using Spark,\n",
    "1) It reads the table [Youtube UGC dataset](https://media.withyoutube.com/) from gs://dataproc-metastore-public-binaries/youtube_ucg/\n",
    "2) It calls Vertex AI Gemini API vision pro to generate description from videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Make sure the service account running this notebook has the required permissions:\n",
    "\n",
    "- **Run the notebook**\n",
    "  - AI Platform Notebooks Service Agent\n",
    "  - Notebooks Admin\n",
    "  - Vertex AI Administrator\n",
    "- **Read files from bucket**\n",
    "  - Storage Object Viewer\n",
    "- **Run Dataproc jobs**\n",
    "  - Dataproc Service Agent\n",
    "  - Dataproc Worker\n",
    "- **Call Google APIs**\n",
    "  - Service Usage Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, concat\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.min_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Video descriptions generation\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BINARIES_BUCKET_PATH = \"gs://dataproc-metastore-public-binaries/youtube_ucg/\"\n",
    "binaries_df = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(BINARIES_BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's select the paths of the first 5 youtube videos\n",
    "paths_df = binaries_df.select(\"path\").limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Define UDF and call Gemini API to generate video descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_descriptions(gcs_uri):\n",
    "\n",
    "  def gemini_predict(gcs_uri, prompt):\n",
    "      \n",
    "    model_url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{project_id}/locations/us-central1/publishers/google/models/gemini-pro-vision:streamGenerateContent\"\n",
    "    request_body = {\n",
    "      \"contents\": {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "          {\n",
    "            \"fileData\": {\n",
    "              \"mimeType\": \"video/mp4\",\n",
    "              \"fileUri\": gcs_uri\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"text\": prompt\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"safety_settings\": {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
    "      },\n",
    "      \"generation_config\": {\n",
    "        \"temperature\": 0.4,\n",
    "        \"topP\": 1.0,\n",
    "        \"topK\": 32,\n",
    "        \"maxOutputTokens\": 2048\n",
    "      }\n",
    "    }\n",
    "      \n",
    "    prediction = requests.post(\n",
    "      model_url,\n",
    "      headers={'Authorization': 'Bearer %s' % credentials.token,\n",
    "               'Content-Type': 'application/json'},\n",
    "      json = request_body\n",
    "    ).json()\n",
    "\n",
    "\n",
    "    full_prediction = \"\"\n",
    "    for pred in prediction:\n",
    "      if \"candidates\" in pred:\n",
    "        content = pred[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        full_prediction += content\n",
    "    return full_prediction\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "        Create a short description for this video with the following questions:\n",
    "         1-) Where the video was from? \n",
    "         2-) How many people we have? \n",
    "         3-) What pople are doing? \n",
    "         4-) whats the proposition for the video?\n",
    "         5-) A sumary description from the itens 1,2,3 and 4\n",
    "        Format the 5 descriptions in a JSON format with the KEYS: Where, HowManyPeople, Task, Proposition and Description.\n",
    "    \"\"\"\n",
    "\n",
    "  descriptions = gemini_predict(gcs_uri, prompt)\n",
    "\n",
    "  return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generate_descriptions_udf = udf(generate_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_descriptions = paths_df.sort(paths_df.path.asc()).withColumn(\"data\", generate_descriptions_udf(paths_df.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_descriptions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract feature from generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('Where', StringType(), True),\n",
    "        StructField('HowManyPeople', StringType(), True),\n",
    "        StructField('Proposition', StringType(), True),\n",
    "        StructField('Description', StringType(), True),\n",
    "        StructField('Task', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "df_final = df_descriptions.withColumn(\"exploded_data\", from_json(regexp_replace(regexp_replace(col(\"data\"),\"json\", \"\"),\"```\",\"\"), schema))\\\n",
    "    .select(col('path'),col('exploded_data.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                path|               Where|       HowManyPeople|         Proposition|         Description|                Task|\n",
    "|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n",
    "|gs://dataproc-met...|The video was rec...|There are two peo...|The video is abou...|The video is abou...|The people are pl...|\n",
    "|gs://dataproc-met...|The video was tak...|There is one pers...|The video is abou...|The video is abou...|The person is mak...|\n",
    "|gs://dataproc-met...|From a notebook p...|          One person|How to draw a bet...|This video shows ...|Drawing some line...|\n",
    "|gs://dataproc-met...|The video was tak...|There is one pers...|The video is abou...|The video shows a...|The person is usi...|\n",
    "|gs://dataproc-met...|From a couch in a...|There is one pers...|The video is abou...|A woman is sittin...|The person is dra...|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teste4 on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000337f1964"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

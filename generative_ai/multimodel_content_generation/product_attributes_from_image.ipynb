{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate product attributes and descriptions from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows how to generate attributes and descriptions of products based on product images in a GCS bucket.  \n",
    "It uses the [Stanford Online Products dataset](https://cvgl.stanford.edu/projects/lifted_struct/) and uses the Vertex AI Imagen for [Captioning](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/image-captioning) & [VQA](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/visual-question-answering) model to generate product attributes.  \n",
    "It uses the [Gemini](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini) to generate product sales descriptions, using Spark UDFs to parallelize processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### **Steps**\n",
    "Using Spark,\n",
    "1) It reads a metadata table of the [Stanford Online Products dataset](https://cvgl.stanford.edu/projects/lifted_struct/) from the **public_datasets** dataset located in the [metastore](../../public_datasets/dataproc_metastore/metastore_public_datasets_quickstart.ipynb) (notebook should be connected with the public metastore if using this specific dataset).\n",
    "This metadata table contains the paths of the image files in the bucket.\n",
    "If you want to apply this to a different dataset, you can read the pdf files in your bucket with spark.read.format(\"binaryFile\") (no need of the metastore) - more details [here](../../public_datasets/dataproc_metastore/metastore_public_datasets_quickstart.ipynb).\n",
    "2) It calls Vertex AI Imagen for Captioning and VQA to get product attributes for each image.\n",
    "3) It calls Vertex AI Gemini API to get product sales descriptions based on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Make sure the service account running this notebook has the required permissions:\n",
    "\n",
    "- **Run the notebook**\n",
    "  - AI Platform Notebooks Service Agent\n",
    "  - Notebooks Admin\n",
    "  - Vertex AI Administrator\n",
    "- **Read tables from Dataproc Metastore**\n",
    "  - Dataproc Metastore Editor\n",
    "  - Dataproc Metastore Metadata Editor\n",
    "  - Dataproc Metastore Metadata User\n",
    "  - Dataproc Metastore Service Agent\n",
    "- **Read files from bucket**\n",
    "  - Storage Object Viewer\n",
    "- **Run Dataproc jobs**\n",
    "  - Dataproc Service Agent\n",
    "  - Dataproc Worker\n",
    "- **Call Google APIs**\n",
    "  - Service Usage Consumer\n",
    "- **BigQuery**\n",
    "  - BigQuery Data Editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, concat\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.min_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Image attributes and descriptions generation\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Read the dataset from the public Dataproc Metastore connected\n",
    "binaries_df = spark.read.table(\"public_datasets.stanford_online_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### Another option is to read from the bucket directly\n",
    "# BINARIES_BUCKET_PATH = \"gs://dataproc-metastore-public-binaries/stanford_online_products/\"\n",
    "# binaries_df = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(BINARIES_BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's select the paths of the first 100 product images\n",
    "paths_df = binaries_df.select(\"path\").limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Define prompts to get image attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompt_color = \"What is the product colors?\"\n",
    "prompt_gender = \"The product shown in the image is most appropriate to be used by men, woman or both?\"\n",
    "prompt_brand = \"What is the brand of the product shown in the image? reply unanswerable if you do not know for sure\"\n",
    "prompt_style = \"What is the style of the product shown in the image? ex: modern, casual, tech\"\n",
    "prompt_material = \"What is the material of the product shown in the image? ex: steel, wood, rubber\"\n",
    "prompt_purpose = \"What is the purpose or usage of this product?\"\n",
    "prompt_year = \"What is the year of the product? reply unanswerable if you do not know for sure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Define UDF and call Image Captioning and VQA APIs to generate product attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def visual_qa(prompt, gcs_uri):\n",
    "\n",
    "  model_url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{project_id}/locations/us-central1/publishers/google/models/imagetext:predict\"\n",
    "\n",
    "  request = {\n",
    "      \"instances\": [\n",
    "        {  \"prompt\": prompt,\n",
    "            \"image\": {\n",
    "                 \"gcsUri\": gcs_uri\n",
    "            }\n",
    "        }\n",
    "      ],\n",
    "      \"parameters\": {\n",
    "        \"sampleCount\": 1\n",
    "      }\n",
    "  }\n",
    "    \n",
    "  if prompt == \"\": # passing no prompt will trigger the image-captioning to get image description instead of visual-question-answering\n",
    "    del request[\"instances\"][0][\"prompt\"] \n",
    "      \n",
    "  prediction = requests.post( model_url,\n",
    "    headers={'Authorization': 'Bearer %s' % credentials.token,\n",
    "             'x-goog-user-project': project_id,\n",
    "             'Content-Type': 'application/json; charset=utf-8'},\n",
    "    json=request\n",
    "  ).json()\n",
    "    \n",
    "  if \"predictions\" in prediction:\n",
    "    return prediction[\"predictions\"][0]\n",
    "  else:\n",
    "    if \"error\" in prediction:\n",
    "      if prediction[\"error\"][\"code\"] == 429:  # Quota exceeded\n",
    "        time.sleep(5)\n",
    "        return visual_qa(prompt, gcs_uri)\n",
    "      else:\n",
    "        return f\"Error getting prediction: {prediction['error']}\"\n",
    "    return f\"Error getting predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "visual_qa_udf = udf(visual_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_metadata_df = paths_df.withColumn(\"description\", visual_qa_udf(lit(\"\"), col(\"path\"))) \\\n",
    "  .withColumn(\"color\", visual_qa_udf(lit(prompt_color), col(\"path\"))) \\\n",
    "  .withColumn(\"gender\", visual_qa_udf(lit(prompt_gender), col(\"path\"))) \\\n",
    "  .withColumn(\"brand\", visual_qa_udf(lit(prompt_brand), col(\"path\"))) \\\n",
    "  .withColumn(\"style\", visual_qa_udf(lit(prompt_style), col(\"path\"))) \\\n",
    "  .withColumn(\"material\", visual_qa_udf(lit(prompt_material), col(\"path\"))) \\\n",
    "  .withColumn(\"purpose\", visual_qa_udf(lit(prompt_purpose), col(\"path\"))) \\\n",
    "  .withColumn(\"year\", visual_qa_udf(lit(prompt_year), col(\"path\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata_df.show(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_metadata_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Define UDF and call Gemini API to generate product sales descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_descriptions(gcs_uri, description, color, gender, brand, style, material, purpose, year):\n",
    "\n",
    "  def gemini_predict(gcs_uri, prompt):\n",
    "      \n",
    "    model_url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{project_id}/locations/us-central1/publishers/google/models/gemini-pro-vision:streamGenerateContent\"\n",
    "    request_body = {\n",
    "      \"contents\": {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "          {\n",
    "            \"fileData\": {\n",
    "              \"mimeType\": \"image/jpeg\",\n",
    "              \"fileUri\": gcs_uri\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"text\": prompt\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"safety_settings\": {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
    "      },\n",
    "      \"generation_config\": {\n",
    "        \"temperature\": 0.4,\n",
    "        \"topP\": 1.0,\n",
    "        \"topK\": 32,\n",
    "        \"maxOutputTokens\": 2048\n",
    "      }\n",
    "    }\n",
    "      \n",
    "    prediction = requests.post(\n",
    "      model_url,\n",
    "      headers={'Authorization': 'Bearer %s' % credentials.token,\n",
    "               'Content-Type': 'application/json'},\n",
    "      json = request_body\n",
    "    ).json()\n",
    "\n",
    "\n",
    "    full_prediction = \"\"\n",
    "    for pred in prediction:\n",
    "      if \"candidates\" in pred:\n",
    "        content = pred[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        full_prediction += content\n",
    "    return full_prediction\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "        You are a retail expert and knows how to write beatiful, elegant and concise product descriptions, based on data about the product.\n",
    "        Based on the PRODUCT DATA, and the image of the product, you are able to provide the PRODUCT SALES DESCRIPTION.\n",
    "\n",
    "        Here is one EXAMPLE:\n",
    "\n",
    "        START____________________________________\n",
    "        PRODUCT DATA:\n",
    "        Product description: Brown Fashion Sneakers\n",
    "        Color: brown\n",
    "        Gender: Women\n",
    "        Brand: NONE\n",
    "        Style: Fashion Flat heel\n",
    "        Material: Polyurethane\n",
    "\n",
    "        PRODUCT SALES DESCRIPTION:\n",
    "        A pair of pink sneakers with white soles on a white background is a stylish and comfortable choice for women who want to add a touch of color to their wardrobe. These sneakers are made of polyurethane, which is a durable and lightweight material that will keep your feet comfortable all day long. The flat heel makes them easy to wear for all-day activities, and the lace-up closure ensures a secure fit.\n",
    "        These sneakers are perfect for a variety of occasions, from running errands to running errands. They can be dressed up or down, depending on your personal style. Pair them with a casual dress or jeans for a relaxed look, or dress them up with a skirt or pants for a more formal look.\n",
    "        If you are looking for a stylish and comfortable pair of sneakers, these pink sneakers with white soles are a great option. They are made of durable materials, are easy to wear, and can be dressed up or down.\n",
    "\n",
    "        You are a retail expert and knows how to write beatiful, elegant and concise product descriptions, based on data about the product.\n",
    "        Based on the PRODUCT DATA, and the image of the product, you are able to provide the PRODUCT SALES DESCRIPTION.\n",
    "        \n",
    "        Generate a PRODUCT SALES DESCRIPTION for this product:\n",
    "\n",
    "        START____________________________________\n",
    "        PRODUCT DATA:\n",
    "        Product description: {description}\n",
    "        Color: {color}\n",
    "        Gender: {gender}\n",
    "        Brand: {brand}\n",
    "        Style: {style}\n",
    "        Material: {material}\n",
    "        Purpose: {purpose}\n",
    "        Year: {year}\n",
    "\n",
    "        PRODUCT SALES DESCRIPTION:\n",
    "    \"\"\"\n",
    "\n",
    "  descriptions = gemini_predict(gcs_uri, prompt)\n",
    "\n",
    "  return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generate_descriptions_udf = udf(generate_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_descriptions_df = image_metadata_df.withColumn(\"sales_description\", generate_descriptions_udf(\"path\", \"description\", \"color\", \"gender\", \"brand\", \"style\", \"material\", \"purpose\", \"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_descriptions_df.sort(image_descriptions_df.path.asc()).withColumn(\"url\", regexp_replace(concat(lit(\"https://storage.mtls.cloud.google.com/\"),col(\"path\")), \"gs://\", \"\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                          path|                   description|color|gender|       brand| style|material|     purpose|        year|             sales_description|                           url|\n",
    "|------------------------------|------------------------------|-----|------|------------|------|--------|------------|------------|------------------------------|------------------------------|\n",
    "|gs://dataproc-metastore-pub...|a kitchen with wooden cabin...|brown| women|unanswerable|modern|    wood|     kitchen|unanswerable| This beautiful kitchen is ...|https://storage.mtls.cloud....|\n",
    "|gs://dataproc-metastore-pub...|a close up of a wooden pill...|brown| women|unanswerable|modern|    wood|     cabinet|unanswerable| This beautiful wooden cabi...|https://storage.mtls.cloud....|\n",
    "|gs://dataproc-metastore-pub...|a crystal chandelier is han...|clear| women|unanswerable|modern|   metal|  decoration|unanswerable| This stunning crystal chan...|https://storage.mtls.cloud....|\n",
    "|gs://dataproc-metastore-pub...|a lamp with a giraffe shape...|white| women|unanswerable|modern|    wood|        lamp|unanswerable| This elegant lamp is sure ...|https://storage.mtls.cloud....|\n",
    "|gs://dataproc-metastore-pub...|a brown and white crocheted...|white| women|unanswerable|modern|  rubber|unanswerable|unanswerable| This crocheted brown and w...|https://storage.mtls.cloud....|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 on cluster-with-metastore-bq (Remote)",
   "language": "python",
   "name": "9b6844836fbacc61a012fc97-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

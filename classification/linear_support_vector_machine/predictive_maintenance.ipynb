{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance for Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to predict if a machine will fail or not using SparkML's Linear SVM Classifier\n",
    "\n",
    "#### **Steps**\n",
    "Using Spark, \n",
    "1) It reads the table [AI4I 2020 Predictive Maintenance Dataset](https://doi.org/10.24432/C5HS5C) from the **public_datasets** dataset located in the [metastore](../gcp_services/README.md) (notebook should be connected with the public metastore if using this specific dataset).       \n",
    "2) It parses process the dataset to choose features and train the ML model (fits the classification model) to predict a target value.  \n",
    "   **Features**: air temperature [K], process temperature [K], rotational speed [rpm], torque [Nm], tool wear [min]\n",
    "   **Target**: machine failure\n",
    "3) It evaluates and plot the results.  \n",
    "\n",
    "#### **Details of the dataset**\n",
    "\n",
    "- Since real predictive maintenance datasets are generally difficult to obtain and in particular difficult to publish, this dataset presents and provides a synthetic dataset that reflects real predictive maintenance encountered in industry to the best of knowledge.\n",
    "- There are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity and Access Management (IAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the service account running this notebook has the required permissions:\n",
    "\n",
    "- **Run the notebook**\n",
    "  - AI Platform Notebooks Service Agent\n",
    "  - Notebooks Admin\n",
    "  - Vertex AI Administrator\n",
    "- **Read tables from Dataproc Metastore**\n",
    "  - Dataproc Metastore Editor\n",
    "  - Dataproc Metastore Metadata Editor\n",
    "  - Dataproc Metastore Metadata User\n",
    "  - Dataproc Metastore Service Agent\n",
    "- **Read files from bucket**\n",
    "  - Storage Object Viewer\n",
    "- **Run Dataproc jobs**\n",
    "  - Dataproc Service Agent\n",
    "  - Dataproc Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Linear SVM Predictive Maintenance\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = spark.read.table(\"public_datasets.ai4i_2020_predictive_maintenance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the count of each class\n",
    "class_counts = raw_dataset.groupBy('machine_failure').count()\n",
    "\n",
    "# Calculate and display the class distribution\n",
    "total_count = raw_dataset.count()\n",
    "class_counts.withColumn('Percentage', (class_counts['count'] / total_count) * 100).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|machine_failure|count|Percentage|\n",
    "|---------------|-----|----------|\n",
    "|              0| 9661|     96.61|\n",
    "|              1|  339|      3.39|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not relevant\n",
    "filtered_dataset = raw_dataset.drop(\"udi\", \"product_id\")\n",
    "filtered_dataset = filtered_dataset.drop(\"twf\",\"hdf\", \"pwf\", \"osf\", \"rnf\") # we don't need types of failure\n",
    "\n",
    "# convert numerical features to float\n",
    "for column in filtered_dataset.columns:\n",
    "  if column != \"type\":\n",
    "    filtered_dataset = filtered_dataset.withColumn(column, filtered_dataset[column].cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer to convert string labels to numerical labels\n",
    "type_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features into a single column\n",
    "feature_cols = [\"type_index\", \"air_temperature_k\", \"process_temperature_k\", \"rotational_speed_rpm\", \"torque_nm\", \"tool_wear_min\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to execute the preprocessing and modeling steps\n",
    "prep_pipeline = Pipeline(stages=[type_indexer, assembler, scaler])\n",
    "\n",
    "processed_dataset = prep_pipeline.fit(filtered_dataset).transform(filtered_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = processed_dataset.select(\"scaled_features\", \"machine_failure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|     scaled_features|machine_failure|\n",
    "|--------------------|---------------|\n",
    "|[0.74437552188095...|            0.0|\n",
    "|[-0.7452693087793...|            0.0|\n",
    "|[-0.7452693087793...|            0.0|\n",
    "|[-0.7452693087793...|            0.0|\n",
    "|[-0.7452693087793...|            0.0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = dataset.randomSplit([0.8, 0.2], seed=24)\n",
    "\n",
    "# Create a Linear Support Vector Machine (SVM) model\n",
    "svm = LinearSVC(maxIter=100, regParam=0.01, labelCol=\"machine_failure\", featuresCol=\"scaled_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[svm])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"machine_failure\")\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the Area Under ROC\n",
    "print(\"Area Under ROC: \" + str(area_under_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame for confusion matrix\n",
    "predictions_pd = predictions.select(\"machine_failure\", \"prediction\").toPandas()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(predictions_pd[\"machine_failure\"], predictions_pd[\"prediction\"])\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "class_names = [\"No failure\", \"Failure\"]\n",
    "plot_confusion_matrix(confusion, classes=class_names, title=\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imbalanced dataset\n",
    "\n",
    "Since the dataset is heavily imbalanced and does not represent **Failure** class properly, we will over-sample this minority class in `machine_failure`.\n",
    "\n",
    "We will use `SMOTE` for over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "pandas_df = dataset.select(\"scaled_features\", \"machine_failure\").toPandas()\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(pandas_df[\"scaled_features\"].apply(lambda x: x.toArray()).values.tolist(), pandas_df[\"machine_failure\"])\n",
    "\n",
    "# Create a new DataFrame from the resampled data\n",
    "resampled_dataset = spark.createDataFrame(pd.DataFrame({\"scaled_features\": X_resampled, \"machine_failure\": y_resampled}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the count of each class\n",
    "class_counts = resampled_dataset.groupBy('machine_failure').count()\n",
    "\n",
    "# Calculate and display the class distribution\n",
    "total_count = resampled_dataset.count()\n",
    "class_counts.withColumn('Percentage', (class_counts['count'] / total_count) * 100).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|machine_failure|count|Percentage|\n",
    "|---------------|-----|----------|\n",
    "|            0.0| 9661|      50.0|\n",
    "|            1.0| 9661|      50.0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert ArrayType to VectorUDT\n",
    "array_to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "resampled_dataset = resampled_dataset.withColumn(\"scaled_features\", array_to_vector_udf(\"scaled_features\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = resampled_dataset.randomSplit([0.8, 0.2], seed=24)\n",
    "\n",
    "# Create a Linear Support Vector Machine (SVM) model\n",
    "svm = LinearSVC(maxIter=100, regParam=0.01, labelCol=\"machine_failure\", featuresCol=\"scaled_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[svm])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"machine_failure\")\n",
    "area_under_roc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the Area Under ROC\n",
    "print(\"Area Under ROC: \" + str(area_under_roc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame for confusion matrix\n",
    "predictions_pd = predictions.select(\"machine_failure\", \"prediction\").toPandas()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(predictions_pd[\"machine_failure\"], predictions_pd[\"prediction\"])\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "class_names = [\"No failure\", \"Failure\"]\n",
    "plot_confusion_matrix(confusion, classes=class_names, title=\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark on cluster-with-metastore-bq (Remote)",
   "language": "python",
   "name": "9b6844836fbacc61a012fc97-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

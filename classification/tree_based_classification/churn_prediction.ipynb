{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c588821-a31c-4d2a-8287-b7b671ee116d",
   "metadata": {},
   "source": [
    "## Goal & Problem Statement\n",
    "The goal of this notebook is to provide a solution for propensity modeling to predict user churn on GA4 data using Spark ML. The dataset used is \"Flood it!\" which is available publically and based on users' demographics and activities within the first 24 hours of app installation we will predict the propensity to churn (1) or not churn (0) using gradient boost classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5836fc-1de0-4c56-a8b4-c18ebc531037",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Prepare the training data using demographic, behavioral data, and the label (churn/not-churn)\n",
    "2. Preprocess the raw events data to identify users and the label features.\n",
    "3. Process demographic and behavorial features.\n",
    "4. Train classification models using Spark ML\n",
    "5. Evaluate classification models using Spark ML\n",
    "6. Make predictions on which users will churn using Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c4baa-bcaa-4583-9d9a-f0b46d4b1aaa",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "This notebook uses this public BigQuery dataset, contains raw event data from a real mobile gaming app called Flood It! (Android app, iOS app). The data schema originates from Google Analytics for Firebase, but is the same schema as Google Analytics 4; this notebook applies to use cases that use either Google Analytics for Firebase or Google Analytics 4 data.\n",
    "\n",
    "Google Analytics 4 (GA4) uses an event-based measurement model. Events provide insight on what is happening in an app or on a website, such as user actions, system events, or errors. Every row in the dataset is an event, with various characteristics relevant to that event stored in a nested format within the row. While Google Analytics logs many types of events already by default, developers can also customize the types of events they also wish to log.\n",
    "\n",
    "ToDo: Dataproc Templates to get the public data into BigQuery (Refer - https://support.google.com/analytics/answer/9823238#zippy=%2Cin-this-article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806763f-0f22-4192-b4db-c407ad553aaf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fc9af-bd5d-4f81-8a79-f7fb7e4f0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and dependencies\n",
    "# !pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e750b8a-5f5a-4ecd-9b88-b0e6aeaec22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sql_func\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.classification as classification\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f70a6e-bd96-4c1b-88d8-0e726d64f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials to authenticate with Google APIs\n",
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bbce1-ef96-4d18-b17b-0a8ad78bb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Churn Prediction - Spark ML\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e243a65-abbb-4d00-893e-22195bd838c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data from bigquery\n",
    "# Public bigquery dataset location: firebase-public-project.analytics_153293282.events_20181003\n",
    "# Public metstore dataset location: dataproc-workspaces-notebooks.propensity_churn_dataset.churn_events\n",
    "\n",
    "events_data_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .load(\"firebase-public-project.analytics_153293282.events_20181003\")\n",
    "events_data_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef223f4-c7d8-440a-9c4c-1ce2eb2e0760",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "It is always helpful to take a look at the overall schema of the data. Here, we look at the overall schema of Google Analytics 4 data which uses an event based measurement model with each row as an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac091b9-bd2f-4cf5-a8e7-8f6c96e2be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Print the overall schema used in Google Analytics 4 as \n",
    "it is a event based measurement model and each row in this dataset is an event. \"\"\"\n",
    "events_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef775d-1b65-4b54-8b24-d789568a973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of users\n",
    "events_data_df.select(countDistinct(\"user_pseudo_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70efa8d-35a1-431b-bcff-ec449b877c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of events\n",
    "events_data_df.agg({'event_timestamp':'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f4790-371f-47ff-91d0-6995de25cc4a",
   "metadata": {},
   "source": [
    "__Findings:__ Certain columns are nested records, there are about 4k users and 50K events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b50df-0ce1-40e2-b1ad-e3c3b6c5f78e",
   "metadata": {},
   "source": [
    "## Prepare the training data using demographic, behavioral data, and the label (churn/not-churn)\n",
    "\n",
    "To predict which user is going to churn or return, the ideal training data format for classification should look like the following:\n",
    "\n",
    "User ID\tUser demographic data\tUser behavioral data\tChurned\n",
    "User1\t(e.g., country, device_type)\t(e.g., # of times they did something within a time period)\t1\n",
    "User2\t(e.g., country, device_type)\t(e.g., # of times they did something within a time period)\t0\n",
    "User3\t(e.g., country, device_type)\t(e.g., # of times they did something within a time period)\t1\n",
    "\n",
    "Characteristics of the training data:\n",
    "\n",
    "- each row is a separate unique user ID\n",
    " - feature(s) for demographic data\n",
    "- feature(s) for behavioral data\n",
    "- the actual label that you want to train the model to predict (e.g., 1 = churned, 0 = returned)\n",
    "- You can train a model with only demographic data or behavioral data, but having a combination of both will likely help you create a more predictive model. For this reason, in this section, you will learn how to pre-process the raw data to follow this training data format.\n",
    "\n",
    "The following sections will walk you through preparing the demographic data, behavioral data, and the label before joining them all together as the training data.\n",
    "\n",
    "1. Identifying the label for each user (churned or returned)\n",
    "2. Extracting demographic data for each user\n",
    "3. Extracting behavioral data for each user\n",
    "4. Combining the label, demographic and behavioral data together as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b01b847-68ec-4fdd-b912-caa20c8f005d",
   "metadata": {},
   "source": [
    "#### Step 1: Identify the label for each user (churned or returned)\n",
    "\n",
    "Here we create the label for each user as churned or returned based on the existing columns as the raw dataset doesn't have a feature that simply identifies users. There are many ways to define user churn, but we will predict 1-day churn as users who do not come back and use the app again after 24 hr of the user's first engagement.\n",
    "\n",
    "In other words, after 24 hr of a user's first engagement with the app:\n",
    "- if the user shows no event data thereafter, the user is considered churned.\n",
    "- if the user does have at least one event datapoint thereafter, then the user is considered returned\n",
    "\n",
    "We will also remove users who were unlikely to have ever returned anyway after spending just a few minutes with the app, which is sometimes referred to as \"bouncing\". Therefore, here \"any user who spent at least 10 minutes on the app, but after 24 hour from when they first engaged with the app, never used the app again\" is a churned user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2f0c9-8b4e-48b9-a412-cc57cd6c10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a local temporary view with events_data_df. \n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\"\"\"\n",
    "\n",
    "events_data_df.createOrReplaceTempView(\"temp_events_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605fb53-8ced-4e03-a334-8f4962cecdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SQL, since the raw data contains all of the events for every user, from their first touch (app installation) to their last touch, \n",
    "# we will use this information to create two columns: churned and bounced '''\n",
    "\n",
    "returning_users = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            user_pseudo_id,\n",
    "            user_first_engagement,\n",
    "            user_last_engagement,\n",
    "            (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,\n",
    "            IF (user_last_engagement < (user_first_engagement + 86400000000), 1, 0) AS churned,\n",
    "            IF (user_last_engagement <= (user_first_engagement + 600000000), 1, 0) AS bounced,\n",
    "            EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month,\n",
    "            EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek\n",
    "        FROM\n",
    "            (SELECT\n",
    "                user_pseudo_id,\n",
    "                MIN(event_timestamp) AS user_first_engagement,\n",
    "                MAX(event_timestamp) AS user_last_engagement\n",
    "            FROM\n",
    "                temp_events_data\n",
    "            WHERE \n",
    "                event_name=\"user_engagement\"\n",
    "            GROUP BY\n",
    "                user_pseudo_id) \n",
    "            AS first_last_touch_table\n",
    "        GROUP BY\n",
    "            user_pseudo_id,\n",
    "            user_first_engagement,\n",
    "            user_last_engagement \"\"\")\n",
    "returning_users.createOrReplaceTempView(\"returning_users\")\n",
    "returning_users.show(1)\n",
    "\n",
    "# Note: could not get dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82221301-000f-4f4e-b12e-d1e0f0ff7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the count of users bounced and returned\n",
    "spark.sql(\"\"\"\n",
    "            SELECT\n",
    "                bounced,\n",
    "                churned, \n",
    "                COUNT(churned) as count_users\n",
    "            FROM\n",
    "                returning_users\n",
    "            GROUP BY 1,2\n",
    "            ORDER BY bounced\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c49928-9f98-4d19-bdea-f5b34e878274",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(CASE WHEN churned = 1 THEN 1 ELSE NULL END) / COUNT(*) AS churn_rate\n",
    "    FROM\n",
    "        returning_users\n",
    "    WHERE\n",
    "      bounced = 0\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74dd5e-d654-4bf3-8668-f9e77d69cd76",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Extracting demographic data for each user\n",
    "\n",
    "Extract the demographic information for each user. Different demographic information about the user is available in the dataset already, including app_info, device, ecommerce, event_params, geo. Demographic features can help the model predict whether users on certain devices or countries are more likely to churn.\n",
    "\n",
    "__Note:__ User's demographics may occasionally change (e.g. moving from one country to another). For simplicity, we will just use the demographic information that Google Analytics 4 provides when the user first engaged with the app as indicated by MIN(event_timestamp). This enables every unique user to be represented by a single row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff157015-6b7e-40b6-9435-8dda1b711e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_demographics = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    user_pseudo_id,\n",
    "    country,\n",
    "    operating_system,\n",
    "    language\n",
    "FROM\n",
    "  (SELECT\n",
    "    user_pseudo_id,\n",
    "    geo.country as country,\n",
    "    device.operating_system as operating_system,\n",
    "    device.language as language,\n",
    "    ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num\n",
    "  FROM\n",
    "    temp_events_data\n",
    "  WHERE\n",
    "    event_name=\"user_engagement\") AS first_values\n",
    "WHERE\n",
    "  row_num = 1\n",
    "\"\"\")\n",
    "user_demographics.createOrReplaceTempView(\"user_demographics\")\n",
    "user_demographics.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf66e5-9d0f-4cc2-839a-6ec7cd942e52",
   "metadata": {},
   "source": [
    "#### Step 3: Extracting behavioral data for each user\n",
    "Here we aggregate and extract behavioral data for each user, resulting in one row of behavioral data per unique user.\n",
    "Since the end goal of this notebook is to predict, based on a user's activity within the first 24 hrs of app installation and whether that user will churn or return thereafter, therefore, we use behavioral data from the first 24 hrs in your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970091b7-e430-4b28-b954-a5a157758c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to explore all the unique events that exist in this dataset, based on event_name\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        event_name,\n",
    "        COUNT(event_name) as event_count\n",
    "    FROM\n",
    "        temp_events_data\n",
    "    GROUP BY 1\n",
    "    ORDER BY\n",
    "       event_count DESC\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bf7fd-8a8f-49f4-9d37-aeb7cff74361",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_aggregate_behaviour = spark.sql(\"\"\"\n",
    "SELECT \n",
    "        user_pseudo_id,\n",
    "      SUM(IF(event_name = 'user_engagement', 1, 0)) AS cnt_user_engagement,\n",
    "      SUM(IF(event_name = 'level_start_quickplay', 1, 0)) AS cnt_level_start_quickplay,\n",
    "      SUM(IF(event_name = 'level_end_quickplay', 1, 0)) AS cnt_level_end_quickplay,\n",
    "      SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) AS cnt_level_complete_quickplay,\n",
    "      SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) AS cnt_level_reset_quickplay,\n",
    "      SUM(IF(event_name = 'post_score', 1, 0)) AS cnt_post_score,\n",
    "      SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) AS cnt_spend_virtual_currency,\n",
    "      SUM(IF(event_name = 'ad_reward', 1, 0)) AS cnt_ad_reward,\n",
    "      SUM(IF(event_name = 'challenge_a_friend', 1, 0)) AS cnt_challenge_a_friend,\n",
    "      SUM(IF(event_name = 'completed_5_levels', 1, 0)) AS cnt_completed_5_levels,\n",
    "      SUM(IF(event_name = 'use_extra_steps', 1, 0)) AS cnt_use_extra_steps\n",
    "FROM\n",
    "    (SELECT\n",
    "        e.*\n",
    "    FROM\n",
    "      temp_events_data e\n",
    "    JOIN\n",
    "      returning_users r\n",
    "    ON\n",
    "      e.user_pseudo_id = r.user_pseudo_id\n",
    "    WHERE\n",
    "      e.event_timestamp <= r.ts_24hr_after_first_engagement) AS users_event_table\n",
    "GROUP BY 1 \"\"\")\n",
    "user_aggregate_behaviour.createOrReplaceTempView(\"user_aggregate_behaviour\")\n",
    "user_aggregate_behaviour.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19660ac-f4ba-425b-b405-84fdc4bb2cfc",
   "metadata": {},
   "source": [
    "#### Step 4: Combining the label, demographic and behavioral data together as training data\n",
    "We now combine the three intermediary views (label, demographic, and behavioral data) into the final training data. \n",
    "\n",
    "__Note:__ you can also specify bounced = 0, in order to limit the training data only to users who did not \"bounce\" within the first 10 minutes of using the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71fd15-bfe7-4ebd-b56b-f2658afb168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dem.*,\n",
    "    IFNULL(beh.cnt_user_engagement, 0) AS cnt_user_engagement,\n",
    "    IFNULL(beh.cnt_level_start_quickplay, 0) AS cnt_level_start_quickplay,\n",
    "    IFNULL(beh.cnt_level_end_quickplay, 0) AS cnt_level_end_quickplay,\n",
    "    IFNULL(beh.cnt_level_complete_quickplay, 0) AS cnt_level_complete_quickplay,\n",
    "    IFNULL(beh.cnt_level_reset_quickplay, 0) AS cnt_level_reset_quickplay,\n",
    "    IFNULL(beh.cnt_post_score, 0) AS cnt_post_score,\n",
    "    IFNULL(beh.cnt_spend_virtual_currency, 0) AS cnt_spend_virtual_currency,\n",
    "    IFNULL(beh.cnt_ad_reward, 0) AS cnt_ad_reward,\n",
    "    IFNULL(beh.cnt_challenge_a_friend, 0) AS cnt_challenge_a_friend,\n",
    "    IFNULL(beh.cnt_completed_5_levels, 0) AS cnt_completed_5_levels,\n",
    "    IFNULL(beh.cnt_use_extra_steps, 0) AS cnt_use_extra_steps,\n",
    "    ret.user_first_engagement,\n",
    "    ret.month,\n",
    "    ret.dayofweek,\n",
    "    ret.churned\n",
    "FROM\n",
    "    returning_users ret\n",
    "LEFT OUTER JOIN\n",
    "    user_demographics dem\n",
    "ON \n",
    "    ret.user_pseudo_id = dem.user_pseudo_id\n",
    "LEFT OUTER JOIN \n",
    "    user_aggregate_behaviour beh\n",
    "ON\n",
    "    ret.user_pseudo_id = beh.user_pseudo_id\n",
    "WHERE \n",
    "    ret.bounced = 0\n",
    "\"\"\")\n",
    "\n",
    "# train_data.createOrReplaceTempView(\"train_data\")\n",
    "# train_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7551122-b760-4d91-92eb-a3513415244f",
   "metadata": {},
   "source": [
    "## Propensity model with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771e7fb-ed62-41eb-ae55-cf4f12d47126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(training_data, test_data) = final_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dd1c4-07b3-4921-97c2-0db8a81eec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f6c45-ee27-421f-9ebd-1aa5ab803e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

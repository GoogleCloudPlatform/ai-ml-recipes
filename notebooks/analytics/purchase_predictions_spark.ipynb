{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "Purchase_Predictions_Spark (2)"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "eBQvliwsLiZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purchase Predictions with PySpark in BigQuery Studio"
      ],
      "metadata": {
        "id": "notebook-title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td><a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/analytics/purchase_predictions_spark.ipynb\"><img src=\"https://avatars.githubusercontent.com/u/33467679?s=200&v=4\" width=\"32px\" alt=\"Colab logo\"> Run in Colab</a></td>\n",
        "  <td><a href=\"https://github.com/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/analytics/purchase_predictions_spark.ipynb\"><img src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\" alt=\"GitHub logo\"> View on GitHub</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/ai-ml-recipes/main/notebooks/analytics/purchase_predictions_spark.ipynb\"><img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> Open in Vertex AI Workbench</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/analytics/purchase_predictions_spark.ipynb\"><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTW1gvOovVlbZAIZylUtf5Iu8-693qS1w5NJw&s\" alt=\"BQ logo\" width=\"35\"> Open in BQ Studio</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fai-ml-recipes%2Fmain%2Fnotebooks/analytics/purchase_predictions_spark.ipynb\"><img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"> Open in Colab Enterprise</a></td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "standard-links"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "TheLook, a hypothetical eCommerce clothing retailer, stores data on customers, products, orders, logistics, web events, and digital marketing campaigns in BigQuery. The company wants to leverage the team's existing SQL and PySpark expertise to analyze this data using Apache Spark.\n",
        "\n",
        "To avoid manual infrastructure provisioning or tuning for Spark, TheLook seeks an auto-scaling solution that allows them to focus on workloads rather than cluster management. Additionally, they want to minimize the effort required to integrate Spark and BigQuery while staying within the BigQuery Studio environment, possibly using BigQuery notebooks.\n",
        "\n",
        "### Approach\n",
        "\n",
        "In this use case, we will demonstrate how to build a logistic regression classification model using PySpark to predict whether a user will make a purchase. The entire workflow is executed within a Colab Enterprise notebook in BigQuery Studio, taking advantage of the built-in serverless Spark engine. This approach allows our data science team to use familiar PySpark libraries for data exploration and model training directly on data stored in BigQuery, creating a seamless experience from data to model within a single, integrated environment.\n"
      ],
      "metadata": {
        "id": "QCcazsyjJvXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "The following steps create resources that will be used throughout the tutorial."
      ],
      "metadata": {
        "id": "RtgdfcrNWNkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enable Google Cloud APIs\n",
        "\n",
        "Enable the necessary Google Cloud APIs for this notebook, and then **refresh the page**."
      ],
      "metadata": {
        "id": "pVU8AVZ5p1cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable cloudaicompanion.googleapis.com dataproc.googleapis.com"
      ],
      "metadata": {
        "id": "Uqq6F2IwXEu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries\n",
        "\n",
        "Install necessary Python libraries."
      ],
      "metadata": {
        "id": "eO48u79FqCs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ $(pip show numpy 2>/dev/null | grep 'Version:' | sed 's/Version: \\([0-9]\\+\\.[0-9]\\+\\).*/\\1/') != \"1.26\" ]; then pip install -U numpy==1.26; fi"
      ],
      "metadata": {
        "id": "NLAcVWG5rXWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Project ID and Location\n",
        "\n",
        "Configure your Google Cloud project ID and a region for resource deployment."
      ],
      "metadata": {
        "id": "YHIbYWxZWxk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\" # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "SzD0y_2iBsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Cloud Storage Bucket\n",
        "\n",
        "Create a [Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) or set an existing one. This bucket will be used for temporary storage by Spark."
      ],
      "metadata": {
        "id": "cYJ_MOKbW3re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-demo\"\n",
        "\n",
        "# storage_client = storage.Client(project=PROJECT_ID)\n",
        "# bucket_obj = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)"
      ],
      "metadata": {
        "id": "4PCe-xdgBoZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Spark Environment\n",
        "\n",
        "### Initialize Dataproc Spark Session\n",
        "\n",
        "This step sets up the Spark environment by importing necessary libraries for connecting to Dataproc and using PySpark. It then creates and configures a Spark Session with specified parameters, providing the `spark` object for subsequent Spark operations.\n",
        "\n",
        "This step can also be accomplished in a single line of code below:\n",
        "\n",
        "\n",
        "spark = DataprocSparkSession.builder.getOrCreate()\n",
        ""
      ],
      "metadata": {
        "id": "RVDKd--rJ-_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "\n",
        "session = Session()\n",
        "\n",
        "session.runtime_config.version = \"2.3\"\n",
        "\n",
        "# You can optionally configure Spark properties as well. See https://cloud.google.com/dataproc-serverless/docs/concepts/properties.\n",
        "session.runtime_config.properties = {\n",
        "  'spark.dynamicAllocation.enabled': 'false',\n",
        "}\n",
        "\n",
        "spark = (\n",
        "    DataprocSparkSession.builder\n",
        "      .appName(\"CustomSparkSession\")\n",
        "      .dataprocSessionConfig(session)\n",
        "      .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "h_tqpdNgg-qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "\n",
        "### Load BigQuery Tables\n",
        "\n",
        "Load the `users` and `order_items` tables from BigQuery into Spark DataFrames and register them as temporary SparkSQL views for easier querying."
      ],
      "metadata": {
        "id": "MxS70d6xVAYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read thelook_ecommerce.users from BigQuery and create a temporary view\n",
        "users = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.thelook_ecommerce.users\").load()\n",
        "users.createOrReplaceTempView(\"users\")\n",
        "\n",
        "# Read thelook_ecommerce.order_items from BigQuery\n",
        "order_items = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.thelook_ecommerce.order_items\").load()\n",
        "order_items.createOrReplaceTempView(\"order_items\")"
      ],
      "metadata": {
        "id": "ClRTDEzzLB8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "BigQuery Studio can leverage Gemini for [advanced code completion capabilities](https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_python_code?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) which can use Natural Language to perform exploratory analysis using SQL and even generate PySpark Code for Feature Engineering.\n",
        "\n",
        "Try the following examples:\n",
        "\n",
        "**Prompt 1**: Use Spark to explore the users table and show the first 10 rows.\n",
        "\n",
        "**Prompt 2**: Use Spark to explore the order_items table and show the first 10 rows.\n",
        "\n",
        "**Prompt 3**: Generate PySpark code to show the top 5 most frequent countries in the users table. Display the country and the number of users from each country.\n",
        "\n",
        "**Prompt 4**: Generate PySpark code to find the average sale price of items in the order_items table.\n",
        "\n",
        "**Prompt 5**: Using the table \"my_dataset.users\", generate code to plot country vs traffic source using a suitable plotting library.\n",
        "\n",
        "**Prompt 6:** Create a histogram showing the distribution of \"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\""
      ],
      "metadata": {
        "id": "3orI5dVOMNdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Users Data\n",
        "\n",
        "Display the first 10 rows of the `users` DataFrame to inspect its schema and data content."
      ],
      "metadata": {
        "id": "explore-users-data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use Spark to explore the users table and show the first 10 rows.\n",
        "\n",
        "users.show(10)"
      ],
      "metadata": {
        "id": "U_B1PuIuW4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Order Items Data\n",
        "\n",
        "Display the first 10 rows of the `order_items` DataFrame to understand its structure."
      ],
      "metadata": {
        "id": "explore-order-items-data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use Spark to explore the order_items table and show the first 10 rows.\n",
        "\n",
        "order_items.show(10)"
      ],
      "metadata": {
        "id": "ekdFabYZXbnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze Top Countries\n",
        "\n",
        "Generate PySpark code to show the top 5 most frequent countries in the `users` table, displaying the country and the number of users from each country."
      ],
      "metadata": {
        "id": "analyze-top-countries"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate PySpark code to show the top 5 most frequent countries in the users table. Display the country and the number of users from each country. All imports should use the Spark connect API, not the regular API.\n",
        "\n",
        "from pyspark.sql.functions import col, count, desc\n",
        "\n",
        "users.groupBy(\"country\").agg(count(\"*\").alias(\"count\")).orderBy(desc(\"count\")).limit(5).show()"
      ],
      "metadata": {
        "id": "7VONSq-SXe1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Average Sale Price\n",
        "\n",
        "Generate code to find the average sale price of items in the `order_items` table."
      ],
      "metadata": {
        "id": "calculate-avg-sale-price"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to find the average sale price of items in the order_items table. All imports should use the Spark connect API, not the regular API.\n",
        "\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "order_items.agg(avg(\"sale_price\")).show()"
      ],
      "metadata": {
        "id": "m1u21g3oX6Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Feature Distributions\n",
        "\n",
        "Create histograms showing the distribution of `age`, `country_hash`, `gender_hash`, and `traffic_source_hash`."
      ],
      "metadata": {
        "id": "visualize-feature-distributions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a histogram showing the distribution of \"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\"\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "\n",
        "def hash_col(df, col):\n",
        "    df[f'{col}_hash'] = df[col].apply(lambda x: int(hashlib.sha256(x.encode('utf-8')).hexdigest(), 16) % 10**8)\n",
        "    return df\n",
        "\n",
        "users_df = users.toPandas()\n",
        "users_df = hash_col(users_df, 'country')\n",
        "users_df = hash_col(users_df, 'gender')\n",
        "users_df = hash_col(users_df, 'traffic_source')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.hist(users_df['age'], bins=20)\n",
        "plt.title('Age Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.hist(users_df['country_hash'], bins=20)\n",
        "plt.title('Country Hash Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.hist(users_df['gender_hash'], bins=20)\n",
        "plt.title('Gender Hash Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.hist(users_df['traffic_source_hash'], bins=20)\n",
        "plt.title('Traffic Source Hash Distribution')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CmuWyKOTrM-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Derive Features and Label\n",
        "\n",
        "In this step, we derive two key columns from the input data:\n",
        "\n",
        "**Creation of features column**:\n",
        "It combines user attributes (age, hashed categorical features) into a numerical array, preparing them for a machine learning model.\n",
        "\n",
        "**Generation of label column:**\n",
        "It creates a binary target variable indicating whether a user has made a purchase or not, derived from order information."
      ],
      "metadata": {
        "id": "pvg1I2Y5QC7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BigQuery dataset with feature engineering in SQL\n",
        "features = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  ARRAY(\n",
        "        CAST(u.age AS DOUBLE),\n",
        "        CAST(hash(u.country) AS BIGINT) * 1.0,\n",
        "        CAST(hash(u.gender) AS BIGINT) * 1.0,\n",
        "        CAST(hash(u.traffic_source) AS BIGINT) * 1.0\n",
        "    ) AS features,\n",
        "    CASE WHEN COALESCE(SUM(oi.sale_price), 0) > 0 THEN 1 ELSE 0 END AS label\n",
        "FROM users AS u\n",
        "LEFT JOIN order_items AS oi\n",
        "ON u.id = oi.user_id\n",
        "GROUP BY u.id, u.age, u.country, u.gender, u.traffic_source;\n",
        "\"\"\")\n",
        "features.show()"
      ],
      "metadata": {
        "id": "Le1JbxZPpEH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Machine Learning Task\n",
        "\n",
        "### Prepare Model and Pipeline\n",
        "\n",
        "This code initializes a StandardScaler to scale the \"features\" column and sets up a LogisticRegression model to predict the binary \"label\" (purchase/no purchase). These components are then chained into a Pipeline for streamlined scaling and model training."
      ],
      "metadata": {
        "id": "-Wj-0bASnK8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.connect.classification import LogisticRegression, LogisticRegressionModel\n",
        "from pyspark.ml.connect.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.connect.feature import StandardScaler\n",
        "from pyspark.ml.connect.pipeline import Pipeline\n",
        "\n",
        "#Split Train and Test Data (80:20)\n",
        "train_data, test_data = features.randomSplit([0.9, 0.1], seed=42)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=30, learningRate=0.1, featuresCol=\"scaled_features\", labelCol=\"label\")\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline(stages=[scaler, lr])"
      ],
      "metadata": {
        "id": "_4UXH0rLm6fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model\n",
        "\n",
        "Fit the defined machine learning pipeline to the training data. This step trains the `StandardScaler` and `LogisticRegression` model sequentially.\n",
        "\n",
        "**Note**: If you see the following logging error, please ignore it: OSError: [Errno 99] Cannot assign requested address"
      ],
      "metadata": {
        "id": "5dzndxJYOPsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "pipeline_model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "G2T4T64rm_97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Predictions on Test Data\n",
        "\n",
        "Apply the trained `pipeline_model` to the `test_data` to generate predictions, and then display the results, which include the original features, label, and the model's predictions."
      ],
      "metadata": {
        "id": "predict-on-test-data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the dataset using the trained model\n",
        "transformed_dataset = pipeline_model.transform(test_data)\n",
        "\n",
        "# Print the new data\n",
        "transformed_dataset.show()"
      ],
      "metadata": {
        "id": "tV2JJ-t1ONES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Install Evaluation Libraries\n",
        "Install the `torcheval` library, which might be used for advanced metrics or as part of a generated code prompt."
      ],
      "metadata": {
        "id": "install-torcheval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "metadata": {
        "id": "P95v-u2ynZln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Area Under Precision-Recall (AUC-PR)\n",
        "\n",
        "This code evaluates the trained model's performance by initializing a `BinaryClassificationEvaluator` to calculate the Area Under the Precision-Recall Curve (AUC-PR) and then computes the score using the model's predictions.\n",
        "\n",
        "This step quantifies the model's ability to distinguish between the two classes (e.g., purchase/no purchase)."
      ],
      "metadata": {
        "id": "7z9R-zu7nWtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "eva = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
        "aucPR = eva.evaluate(transformed_dataset)\n",
        "print(f\"AUC PR: {aucPR}\")"
      ],
      "metadata": {
        "id": "1PIlkb-Unatt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Results\n",
        "\n",
        "Let's visualize the results to see how our model performs, and how it has predicted.\n",
        "\n",
        "### Plot Precision-Recall Curve\n",
        "\n",
        "Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library."
      ],
      "metadata": {
        "id": "m5wsbhsAncHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import pandas as pd\n",
        "\n",
        "# Collect predictions and labels\n",
        "predictions = transformed_dataset.select(\"prediction\").toPandas()\n",
        "labels = transformed_dataset.select(\"label\").toPandas()\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(labels, predictions)\n",
        "\n",
        "# Plot the PR curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bt64dC_voJ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Confusion Matrix\n",
        "\n",
        "Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap and a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)."
      ],
      "metadata": {
        "id": "visualize-confusion-matrix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'transformed_dataset' is a Spark DataFrame with 'label' and 'prediction' columns\n",
        "predictions_pd = transformed_dataset.select('label', 'prediction').toPandas()\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(predictions_pd['label'], predictions_pd['prediction'])\n",
        "\n",
        "# Create a heatmap visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Display the confusion matrix as a table\n",
        "cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
        "print(\"Confusion Matrix (Table):\\n\", cm_df)\n",
        "\n",
        "# Extract TP, TN, FP, FN\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"True Positives (TP): {tp}\")\n",
        "print(f\"True Negatives (TN): {tn}\")\n",
        "print(f\"False Positives (FP): {fp}\")\n",
        "print(f\"False Negatives (FN): {fn}\")"
      ],
      "metadata": {
        "id": "hnekNs7-Hgap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Predictions to BigQuery\n",
        "\n",
        "### Write Predictions to BigQuery\n",
        "\n",
        "Use Spark to write the transformed dataset, which includes the model's predictions, to a new table in BigQuery. This allows for persistent storage and further analysis of the prediction results."
      ],
      "metadata": {
        "id": "ncZ55Z0sxF7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using Spark, write the transformed dataset to BigQuery.\n",
        "\n",
        "# Write the transformed dataset to BigQuery\n",
        "transformed_dataset.write.format(\"bigquery\").option(\"table\", f\"{PROJECT_ID}.my_dataset.predictions\").mode(\"overwrite\").save()"
      ],
      "metadata": {
        "id": "N1U-voG4xajv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
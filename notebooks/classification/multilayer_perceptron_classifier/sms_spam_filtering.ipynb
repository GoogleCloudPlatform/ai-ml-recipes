{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/classification/multilayer_perceptron_classifier/sms_spam_filtering.ipynb\">\n",
    "      <img src=\"https://avatars.githubusercontent.com/u/33467679?s=200&v=4\" width=\"32px\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/classification/multilayer_perceptron_classifier/sms_spam_filtering.ipynb\">\n",
    "      <img src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/ai-ml-recipes/main/notebooks/classification/multilayer_perceptron_classifier/sms_spam_filtering.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/ai-ml-recipes/blob/main/notebooks/classification/multilayer_perceptron_classifier/sms_spam_filtering.ipynb\">\n",
    "      <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTW1gvOovVlbZAIZylUtf5Iu8-693qS1w5NJw&s\" alt=\"BQ logo\" width=\"35\">\n",
    "      Open in BQ Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fai-ml-recipes%2Fmain%2Fnotebooks%2Fclassification%2Fmultilayer_perceptron_classifier%2Fsms_spam_filtering.ipynb\">\n",
    "    <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "    Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to predict if an SMS is spam or not using SparkML's Multilayer Perceptron Classifier\n",
    "\n",
    "#### **Steps**\n",
    "Using Spark, \n",
    "1) It reads the table [SMS Spam Collection](https://doi.org/10.24432/C5CC84) from [gs://dataproc-metastore-public-binaries/sms_spam_collection/](https://console.cloud.google.com/storage/browser/dataproc-metastore-public-binaries/sms_spam_collection) \n",
    "2) It parses process the dataset to choose features and train the ML model (fits the classification model) to predict a target value.  \n",
    "   **Features**: text\n",
    "   **Target**: spam or ham \n",
    "3) It evaluates and plot the results.  \n",
    "\n",
    "#### **Details of the dataset**\n",
    "\n",
    "- A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: http://www.grumbletext.co.uk/.\n",
    "- A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/.\n",
    "- A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity and Access Management (IAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the service account running this notebook has the required permissions:\n",
    "\n",
    "- **Run the notebook**\n",
    "  - AI Platform Notebooks Service Agent\n",
    "  - Notebooks Admin\n",
    "  - Vertex AI Administrator\n",
    "- **Read files from bucket**\n",
    "  - Storage Object Viewer\n",
    "- **Run Dataproc jobs**\n",
    "  - Dataproc Service Agent\n",
    "  - Dataproc Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import dependencies\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SMS spam filtering with Multilayer Perceptron Classifier\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = spark.read.option(\"header\",True).csv(\"gs://dataproc-metastore-public-binaries/sms_spam_collection/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the count of each class\n",
    "class_counts = raw_dataset.groupBy('label').count()\n",
    "\n",
    "# Calculate and display the class distribution\n",
    "total_count = raw_dataset.count()\n",
    "class_counts.withColumn('Percentage', (class_counts['count'] / total_count) * 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|label|count|       Percentage|\n",
    "|-----|-----|-----------------|\n",
    "|  ham| 4827|86.59849300322928|\n",
    "| spam|  747|13.40150699677072|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataset to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer to convert string labels to numerical labels\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the SMS text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing TF to convert words to numerical features\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"numerical_features\", numFeatures=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"numerical_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "processed_dataset = prep_pipeline.fit(raw_dataset).transform(raw_dataset)\n",
    "\n",
    "dataset = processed_dataset.select(\"label_index\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|label_index|            features|\n",
    "|-----------|--------------------|\n",
    "|        0.0|(5000,[98,740,750...|\n",
    "|        0.0|(5000,[1727,2630,...|\n",
    "|        1.0|(5000,[581,587,10...|\n",
    "|        0.0|(5000,[594,862,31...|\n",
    "|        0.0|(5000,[1197,1515,...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create an MLP classifier\n",
    "layers = [5000, 100, 50, 2]  # Input: 5000 features, two hidden layers, output: binary (spam or ham)\n",
    "mlp_classifier = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label_index\",\n",
    "    featuresCol=\"features\",\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[mlp_classifier])\n",
    "\n",
    "# Train the MLP classifier\n",
    "mlp_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = mlp_model.transform(testData)\n",
    "\n",
    "# Evaluate the classifier using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_index\",\n",
    "    rawPredictionCol=\"rawPrediction\",  # Use raw prediction scores\n",
    "    metricName=\"areaUnderROC\"  # You can choose other metrics like \"areaUnderPR\" if needed\n",
    ")\n",
    "\n",
    "areaUnderROC = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Area Under ROC: {areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame for confusion matrix\n",
    "predictions_pd = predictions.select(\"label_index\", \"prediction\").toPandas()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(predictions_pd[\"label_index\"], predictions_pd[\"prediction\"])\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "class_names = [\"ham\", \"spam\"]\n",
    "plot_confusion_matrix(confusion, classes=class_names, title=\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 on cluster-dpms (Remote)",
   "language": "python",
   "name": "7a42ae6b283a613cfabce6f7-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
